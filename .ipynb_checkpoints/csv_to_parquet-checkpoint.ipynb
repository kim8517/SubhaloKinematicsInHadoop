{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539a4795-de79-414e-b1c9-76323f5029f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import h5py\n",
    "#from netCDF4 import Dataset\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pqw\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee3e86e-033b-4009-987f-eb72a64ce8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "plt.rc('font', family='serif') \n",
    "plt.rc('font', serif='Times New Roman') \n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7387e-cf22-4efb-9ba6-a990478f500e",
   "metadata": {},
   "source": [
    "# Initiate a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b18d8ec-834e-4c2b-a108-d68113472f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/21 14:58:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# PySpark packages\n",
    "from pyspark import SparkContext   \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "#spark = SparkSession.builder \\\n",
    "#    .master(\"yarn\") \\\n",
    "#    .appName(\"spark-shell\") \\\n",
    "#    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "#    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "#    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "#    .config(\"spark.executor.cores\", \"1\") \\\n",
    "#    .config(\"spark.executor.instances\", \"30\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "#    .config(\"spark.executor.memory\", \"14g\") \\\n",
    "#    .config(\"spark.executor.cores\", \"2\") \\\n",
    "#    .config(\"spark.executor.instances\", \"60\") \\\n",
    "#    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "#    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.7.0-spark2.4-s_2.11\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"spark://sohnic:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"100g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"hdfs://sohnic:54310/tmp/checkpoints\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 500)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d641535c-8b75-4751-ac40-244f2cc50906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.submitTime', '1740117521067'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.startTime', '1740117521260'),\n",
       " ('spark.master', 'spark://sohnic:7077'),\n",
       " ('spark.driver.port', '40333'),\n",
       " ('spark.app.name', 'MyApp'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/nowan/shape_measurement/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.host', 'sohnic')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the spark configuration\n",
    "sc.getConf().getAll()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212f7b9-f5e3-4ab4-a05a-4a609d4c579c",
   "metadata": {},
   "source": [
    "# Define particle data directories to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c745a50-32ed-4cd6-ba7e-3243ef7275da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of particle data files: 600\n",
      "First five data files:\n",
      "['../TNG100/output/snap_099/sorted/combined/snap099_sorted_x0_y0_z0.csv'\n",
      " '../TNG100/output/snap_099/sorted/combined/snap099_sorted_x0_y0_z1.csv'\n",
      " '../TNG100/output/snap_099/sorted/combined/snap099_sorted_x0_y0_z2.csv'\n",
      " '../TNG100/output/snap_099/sorted/combined/snap099_sorted_x0_y0_z3.csv'\n",
      " '../TNG100/output/snap_099/sorted/combined/snap099_sorted_x0_y0_z4.csv']\n"
     ]
    }
   ],
   "source": [
    "ptl_files = np.sort(glob.glob('/data/TNG300/TNG300/snap_099/snap099_*.csv')) # pathes of particle data files\n",
    "\n",
    "print(f\"The number of particle data files: {len(ptl_files)}\")\n",
    "print(\"First five data files:\")\n",
    "print(ptl_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf45a88-cd9b-4877-9e46-fe03d0a6c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 ms, sys: 17.2 ms, total: 50.5 ms\n",
      "Wall time: 49.1 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "      <th>mass</th>\n",
       "      <th>ID</th>\n",
       "      <th>Formation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4816.950870</td>\n",
       "      <td>943.885776</td>\n",
       "      <td>11228.531968</td>\n",
       "      <td>306.11350</td>\n",
       "      <td>78.891620</td>\n",
       "      <td>114.423680</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>122500826831</td>\n",
       "      <td>0.989542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4817.317051</td>\n",
       "      <td>945.470532</td>\n",
       "      <td>11228.469476</td>\n",
       "      <td>296.45004</td>\n",
       "      <td>84.648110</td>\n",
       "      <td>94.154830</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>121755914935</td>\n",
       "      <td>0.871483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4817.384923</td>\n",
       "      <td>943.359975</td>\n",
       "      <td>11228.005229</td>\n",
       "      <td>276.48822</td>\n",
       "      <td>97.958770</td>\n",
       "      <td>118.081530</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>118229119755</td>\n",
       "      <td>0.591693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4817.536942</td>\n",
       "      <td>944.688338</td>\n",
       "      <td>11229.095310</td>\n",
       "      <td>270.81726</td>\n",
       "      <td>99.322174</td>\n",
       "      <td>101.761420</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>117845451181</td>\n",
       "      <td>0.550120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4816.251953</td>\n",
       "      <td>943.217905</td>\n",
       "      <td>11229.008699</td>\n",
       "      <td>314.01993</td>\n",
       "      <td>72.181700</td>\n",
       "      <td>106.111336</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>119612834109</td>\n",
       "      <td>0.802193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            px          py            pz         vx         vy          vz  \\\n",
       "0  4816.950870  943.885776  11228.531968  306.11350  78.891620  114.423680   \n",
       "1  4817.317051  945.470532  11228.469476  296.45004  84.648110   94.154830   \n",
       "2  4817.384923  943.359975  11228.005229  276.48822  97.958770  118.081530   \n",
       "3  4817.536942  944.688338  11229.095310  270.81726  99.322174  101.761420   \n",
       "4  4816.251953  943.217905  11229.008699  314.01993  72.181700  106.111336   \n",
       "\n",
       "       mass            ID  Formation_time  \n",
       "0  0.000075  122500826831        0.989542  \n",
       "1  0.000081  121755914935        0.871483  \n",
       "2  0.000051  118229119755        0.591693  \n",
       "3  0.000074  117845451181        0.550120  \n",
       "4  0.000055  119612834109        0.802193  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# An example data from a particle data file\n",
    "df = pd.read_csv(ptl_files[0])\n",
    "df.head()\n",
    "#h5f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574b5e3-4c9b-4868-819f-2272f82468ed",
   "metadata": {},
   "source": [
    "# Save selected features as a parquet\n",
    "We will convert all particle data files from .csv to .parquet formats.\n",
    "\n",
    "The blow cells show how to convert one particle data file into .parquet format.\n",
    "\n",
    "If you need to convert all particle data files and do not need to know how to convert them individually, skip this section and go to the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdf90-d06d-4a1b-8047-4322467a2069",
   "metadata": {},
   "source": [
    "## Construct a Spark DataFrame\n",
    "First, we save the particle data from the particle data files into a Spark Data Frame named sparkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be02c85-d053-4608-949c-5734c2980b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data structure for a Spark Data Frame\n",
    "# T.StructType([...]): define a structured schema for a DataFrame\n",
    "# T.StructField(name, data_type, nullable)\n",
    "#  name: column name\n",
    "#  data_type: Data type (IntegerType(), StringType(), ShortType(), DoubleType(), BooleanType(), ...)\n",
    "#  nullable: If True, the column can have Null values\n",
    "schema = T.StructType([\\\n",
    "                       T.StructField('px',T.FloatType(), True),\\\n",
    "                       T.StructField('py',T.FloatType(), True),\\\n",
    "                       T.StructField('pz',T.FloatType(), True),\\\n",
    "                       T.StructField('vx',T.FloatType(), True),\\\n",
    "                       T.StructField('vy',T.FloatType(), True),\\\n",
    "                       T.StructField('vz',T.FloatType(), True),\\\n",
    "                       T.StructField('mass',T.FloatType(), True),\\\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d1c9bd-a82c-4e69-b719-34efc577f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(s.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.1 ms, sys: 11.1 ms, total: 58.1 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate a Spark Data Frame named spakrdf according to the data in the pandas dataframe and the data type of schema\n",
    "sparkdf = spark.createDataFrame(df[['px', 'py', 'pz', 'vx', 'vy', 'vz', 'mass']],schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d4eafd-6171-4f67-a0b1-228be45ddf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three rows of sparkdf:\n",
      "+---------+--------+---------+---------+--------+---------+-----------+\n",
      "|       px|      py|       pz|       vx|      vy|       vz|       mass|\n",
      "+---------+--------+---------+---------+--------+---------+-----------+\n",
      "|4816.9507|943.8858|11228.532| 306.1135|78.89162|114.42368|7.473642E-5|\n",
      "| 4817.317|945.4705| 11228.47|296.45004|84.64811| 94.15483|8.121476E-5|\n",
      "| 4817.385|  943.36|11228.005|276.48822|97.95877|118.08153|5.140894E-5|\n",
      "+---------+--------+---------+---------+--------+---------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "The number of rows: 25271\n",
      "The structure of sparkdf:\n",
      "root\n",
      " |-- px: float (nullable = true)\n",
      " |-- py: float (nullable = true)\n",
      " |-- pz: float (nullable = true)\n",
      " |-- vx: float (nullable = true)\n",
      " |-- vy: float (nullable = true)\n",
      " |-- vz: float (nullable = true)\n",
      " |-- mass: float (nullable = true)\n",
      "\n",
      "None\n",
      "CPU times: user 5.78 ms, sys: 6.49 ms, total: 12.3 ms\n",
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Top three rows of sparkdf:\")\n",
    "sparkdf.show(3,truncate=True)\n",
    "print()\n",
    "\n",
    "nrow = sparkdf.count()\n",
    "print(f\"The number of rows: {nrow}\")\n",
    "print()\n",
    "\n",
    "print(\"The structure of sparkdf:\")\n",
    "print(sparkdf.printSchema())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042ecbf-4070-442a-bda2-ed21377a992f",
   "metadata": {},
   "source": [
    "# Converting all particle files (here, .csv file) into one parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65daedd-1c39-477a-94d8-1bbcef29d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "schema = T.StructType([\\\n",
    "                       T.StructField('px',T.FloatType(), True),\\\n",
    "                       T.StructField('py',T.FloatType(), True),\\\n",
    "                       T.StructField('pz',T.FloatType(), True),\\\n",
    "                       T.StructField('vx',T.FloatType(), True),\\\n",
    "                       T.StructField('vy',T.FloatType(), True),\\\n",
    "                       T.StructField('vz',T.FloatType(), True),\\\n",
    "                       T.StructField('mass',T.FloatType(), True),\\\n",
    "                      ])\n",
    "\n",
    "# create an empty spark DataFrame for saving all particle data\n",
    "sparkdf = spark.createDataFrame([], schema)\n",
    "\n",
    "for i in tqdm(range(len(ptl_files))):\n",
    "    # read i-th data file (.csv) of the particle data files and convert to a spark DataFrame\n",
    "    df = pd.read_csv(ptl_files[i])\n",
    "    tempdf = spark.createDataFrame(df[['px', 'py', 'pz', 'vx', 'vy', 'vz', 'mass']], schema)\n",
    "    # Append to the spark DataFrame for all particle data\n",
    "    sparkdf = sparkdf.union(tempdf)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdc4af-5338-41d3-b1bf-9a2432c4f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "outfile = \"hdfs://sohnic:54310/data/TNG100/output/snap_099/particle.parquety.snappy\" # make sure to start with 'hdfs://sohnic:54310' because we will save it in hadoop directory, not a real directory.\n",
    "sparkdf.write.option(\"compression\", \"snappy\").mode(\"overwrite\").save(outfile)\n",
    "# .option(key, value): set saving configuration.\n",
    "# \"compression\": determines the compression codec for saving the spark DataFrame.\n",
    "# \"snappy\" is a default compression option for Parquet. It is very fast and has a moderate compression ratio.\n",
    "# other options are Gzip, Bzip2, LZ4, ZSTD. Ask ChatGPT if you want to know their properties.\n",
    "# mode: specifies what happens if the output file already exists.\n",
    "# \"overwrite\": overwrite an existing files at the output path.\n",
    "# \"append\": add new data to existing files.\n",
    "# \"ignore\": skips writing if the output file already exists.\n",
    "# \"error\" or \"errorifexists\": raise an error if the output file exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a815904-e9c1-427f-b21f-33f49cfa3599",
   "metadata": {},
   "source": [
    "# Read the saved parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20a421e-e6eb-4812-9a15-f92253d53e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 ms, sys: 0 ns, total: 3.39 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "newsparkdf = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(outname)\n",
    "# hearder==\"true\": use the first row to infer column names. Actually, it is unnecessary if you read a Parquet file because the Parquet file independently stores column names in its format.\n",
    "# recursiveFileLookup=\"true\"\n",
    "#  Allows Spark to recursively search through subdirectories within the specified path (outname) for Parquet files.\n",
    "#  If outname is a directory with nested folders, Spark will read all Parquet files within those folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ba56-93e9-47b1-83d9-0d3b7c6c8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# summarize the spark Data Frame\n",
    "newsparkdf.describe().toPandas()\n",
    "# describe(): summarize the data in the spark Data Frame\n",
    "# .toPandas(): convert the spark Data Frame into a pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53109580-1fde-4d78-a93e-f603608464f8",
   "metadata": {},
   "source": [
    "# Saving subhalo catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5a2b18-f1db-4181-93da-404f3726246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubfindID</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "      <th>StarHalfMass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>43718.812500</td>\n",
       "      <td>48813.640625</td>\n",
       "      <td>147594.953125</td>\n",
       "      <td>472.196198</td>\n",
       "      <td>450.850006</td>\n",
       "      <td>-260.746918</td>\n",
       "      <td>265.473969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>45442.273438</td>\n",
       "      <td>51850.199219</td>\n",
       "      <td>146416.500000</td>\n",
       "      <td>-209.056656</td>\n",
       "      <td>-735.888916</td>\n",
       "      <td>400.641724</td>\n",
       "      <td>126.831070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44490.761719</td>\n",
       "      <td>49091.714844</td>\n",
       "      <td>147870.578125</td>\n",
       "      <td>2021.729492</td>\n",
       "      <td>1495.440186</td>\n",
       "      <td>-1797.082153</td>\n",
       "      <td>28.682310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>43820.785156</td>\n",
       "      <td>50939.398438</td>\n",
       "      <td>147711.046875</td>\n",
       "      <td>925.150391</td>\n",
       "      <td>-473.445465</td>\n",
       "      <td>-275.925934</td>\n",
       "      <td>11.954713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>44302.578125</td>\n",
       "      <td>49630.972656</td>\n",
       "      <td>147869.484375</td>\n",
       "      <td>-260.214630</td>\n",
       "      <td>-2221.625244</td>\n",
       "      <td>-563.641296</td>\n",
       "      <td>11.029386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SubfindID            px            py             pz           vx  \\\n",
       "0          0  43718.812500  48813.640625  147594.953125   472.196198   \n",
       "1          1  45442.273438  51850.199219  146416.500000  -209.056656   \n",
       "2          2  44490.761719  49091.714844  147870.578125  2021.729492   \n",
       "3          3  43820.785156  50939.398438  147711.046875   925.150391   \n",
       "4          4  44302.578125  49630.972656  147869.484375  -260.214630   \n",
       "\n",
       "            vy           vz  StarHalfMass  \n",
       "0   450.850006  -260.746918    265.473969  \n",
       "1  -735.888916   400.641724    126.831070  \n",
       "2  1495.440186 -1797.082153     28.682310  \n",
       "3  -473.445465  -275.925934     11.954713  \n",
       "4 -2221.625244  -563.641296     11.029386  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subhalo table (~few seconds)\n",
    "t300subhalo = pd.read_csv('subhalocat300.txt', sep=' ')\n",
    "t300subhalo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "026e336b-1594-4eab-901a-2b131310c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(s.dtype):\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.79 s, sys: 443 ms, total: 4.24 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58990)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "schema_sub = T.StructType([\\\n",
    "                       T.StructField('px',T.FloatType(), True),\\\n",
    "                       T.StructField('py',T.FloatType(), True),\\\n",
    "                       T.StructField('pz',T.FloatType(), True),\\\n",
    "                       T.StructField('vx',T.FloatType(), True),\\\n",
    "                       T.StructField('vy',T.FloatType(), True),\\\n",
    "                       T.StructField('vz',T.FloatType(), True),\\\n",
    "                       T.StructField('StarHalfMass',T.FloatType(), True),\\\n",
    "                       T.StructField('sub_id',T.IntegerType(), True)\n",
    "                      ])\n",
    "\n",
    "SubhaloDf = spark.createDataFrame(t300subhalo[['px', 'py', 'pz', 'vx', 'vy', 'vz', 'StarHalfMass', 'SubfindID']], schema_sub)\n",
    "SubhaloFile = 'hdfs://sohnic:54310/data/TNG300/snap99/subhalo.parquet.snappy'\n",
    "SubhaloDf.write.option(\"compression\", \"snappy\").mode(\"overwrite\").save(SubhaloFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba84f4-23f5-4782-9029-d4664e70a3f8",
   "metadata": {},
   "source": [
    "# repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b272a485-dbc5-476a-a242-141427929da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 ms, sys: 40 Âµs, total: 4.13 ms\n",
      "Wall time: 743 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "765683"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# similar to \"refresh\"\n",
    "newsparkdf.cache()\n",
    "newsparkdf.repartition(10,\"px\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
