{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475be306-ed84-4c4c-aa20-dac8d6e66e52",
   "metadata": {},
   "source": [
    "We will perfrom various measurements based on all stellar particles within 50 kpc from the subhalo center. To do this, we should construct a data frame that connect the subhalo and the particles from the subhalo centers. This python code construct a Spark Data Frame that joins subhalos and particles within 50 kpc from the subhalo postions and save it in the parquet format.\n",
    "\n",
    "It takes several hours. I recommend to do this task in .py file and run in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c7769-8d5b-41e2-8b87-51a01c96bb29",
   "metadata": {},
   "source": [
    "# 0. Import packages and set Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317a204a-5931-4bec-8bf6-21af394be50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pqw\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "import gc\n",
    "\n",
    "h = 0.6774\n",
    "a = 1/(1+0.62)\n",
    "box_size = 205000\n",
    "t_h = 7.786*1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3fd752-f3dc-470f-a573-b838d6761987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 08:55:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext   \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import broadcast, col, sqrt, pow, floor, monotonically_increasing_id, abs, pmod, least, row_number\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"spark://sohnic:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"100g\")\\\n",
    "    .config(\"spark.driver.memory\", \"100g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"hdfs://sohnic:54310/tmp/checkpoints\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 500)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.hive.filesourcePartitionFileCacheSize\", 524288000) # 500MB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d40ccb-87e4-4b2a-8e16-6c3e83be1d40",
   "metadata": {},
   "source": [
    "# 1. Read the particle and subhalo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f49460-a8a7-4873-bea7-46d249cb2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://sohnic:54310/data/TNG300/snap_099/dm_particle_batch0.parquet\n",
      "hdfs://sohnic:54310/data/TNG300/snap_099/dm_particle_batch1.parquet\n",
      "hdfs://sohnic:54310/data/TNG300/snap_099/dm_particle_batch10.parquet\n",
      "hdfs://sohnic:54310/data/TNG300/snap_099/dm_particle_batch100.parquet\n",
      "hdfs://sohnic:54310/data/TNG300/snap_099/dm_particle_batch101.parquet\n"
     ]
    }
   ],
   "source": [
    "# list parquet files\n",
    "parquet_files = spark._jvm.org.apache.hadoop.fs.FileSystem \\\n",
    "    .get(spark._jsc.hadoopConfiguration()) \\\n",
    "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(\"hdfs://sohnic:54310/data/TNG300/snap_099\"))\n",
    "\n",
    "star_parquets = [\"hdfs://sohnic:54310/data/TNG300/snap_099/\"+f.getPath().getName() for f in parquet_files if 'star_particle_batch' in f.getPath().getName().split('/')[-1]]\n",
    "dm_parquets = [\"hdfs://sohnic:54310/data/TNG300/snap_099/\"+f.getPath().getName() for f in parquet_files if 'dm_particle_batch' in f.getPath().getName().split('/')[-1]]\n",
    "for f in dm_parquets[:5]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c376c867-2f23-4e62-a03f-4b10692aed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 09:07:22 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 524288000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 344 ms, total: 1.93 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read particle data and save in 'ptldf'\n",
    "# particle_file = 'hdfs://sohnic:54310/data/TNG300/snap99/snap099_cubic_indexed.parquet.snappy'\n",
    "\n",
    "# ptldf = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(particle_file)\n",
    "# ptldf.show(4)\n",
    "star_ptldf =  spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(star_parquets[0])\n",
    "for parquet in star_parquets[1:]:\n",
    "    tempdf = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(parquet)\n",
    "    star_ptldf = star_ptldf.union(tempdf)\n",
    "\n",
    "\n",
    "dm_ptldf =  spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(dm_parquets[0])\n",
    "for parquet in dm_parquets[1:]:\n",
    "    tempdf = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(parquet)\n",
    "    dm_ptldf = dm_ptldf.union(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b84812-aea7-4907-9fba-e49f938f4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+----------+----------+------+\n",
      "|      px|        py|       pz|        vx|        vy|        vz|sub_id|\n",
      "+--------+----------+---------+----------+----------+----------+------+\n",
      "|36759.13|101113.055|163010.64| 34.259277| -505.0985| 1089.3732|219327|\n",
      "|35568.41| 102169.96|163246.75|-119.23757|  238.6627|  505.6653|219328|\n",
      "|36117.34| 101716.64|163704.84| 431.59103|-1802.9314| 132.34152|219329|\n",
      "|36200.65| 101865.22|163540.89| 785.96625|-398.43533|-396.46136|219330|\n",
      "+--------+----------+---------+----------+----------+----------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+----------+\n",
      "|subhalo_px|subhalo_py|subhalo_pz|subhalo_vx|subhalo_vy|subhalo_vz|subhalo_id|\n",
      "+----------+----------+----------+----------+----------+----------+----------+\n",
      "|  36759.13|101113.055| 163010.64| 34.259277| -505.0985| 1089.3732|    219327|\n",
      "|  35568.41| 102169.96| 163246.75|-119.23757|  238.6627|  505.6653|    219328|\n",
      "|  36117.34| 101716.64| 163704.84| 431.59103|-1802.9314| 132.34152|    219329|\n",
      "|  36200.65| 101865.22| 163540.89| 785.96625|-398.43533|-396.46136|    219330|\n",
      "+----------+----------+----------+----------+----------+----------+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "CPU times: user 4.18 ms, sys: 10.3 ms, total: 14.5 ms\n",
      "Wall time: 4.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read suhalo data and save in subdf\n",
    "subhalofile = 'hdfs://sohnic:54310/data/TNG300/snap_099/subhalo.parquet.snappy'\n",
    "subdf = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(subhalofile)\n",
    "subdf.show(4)\n",
    "\n",
    "# change the column names to avoid the confunsion in comparision with ptldf.\n",
    "subdf = subdf.withColumnRenamed(\"px\", \"subhalo_px\")\n",
    "subdf = subdf.withColumnRenamed(\"py\", \"subhalo_py\")\n",
    "subdf = subdf.withColumnRenamed(\"pz\", \"subhalo_pz\")\n",
    "subdf = subdf.withColumnRenamed(\"vx\", \"subhalo_vx\")\n",
    "subdf = subdf.withColumnRenamed(\"vy\", \"subhalo_vy\")\n",
    "subdf = subdf.withColumnRenamed(\"vz\", \"subhalo_vz\")\n",
    "subdf = subdf.withColumnRenamed(\"sub_id\", \"subhalo_id\")\n",
    "# .withColumnRenamed(\"column_name1\", \"column_name2\"): replace the name of the \"column_name1\" with \"column_name2\"\n",
    "\n",
    "subdf.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd780d-ff33-4b57-b1ab-ecd803cde45d",
   "metadata": {},
   "source": [
    "We will use subhalos with the stellar mass within a two stellar half mass radius larger than 10^9 M_sun. Thus, we fill the subdf based on the \"StarHalfMass\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fea6cde-f439-4507-ba72-847d5f429927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .filter(A): filter a Saprk Data Frame based on the condition A.\n",
    "# F.col(X): refering the data of a column X from a Spark Data Frame\n",
    "# note that the the unit of \"StarHalfMass\" data is M_sun/h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956ae32-4ba1-491e-be0b-7daaae2cd802",
   "metadata": {},
   "source": [
    "# 2. Saving the particle data within the 50kpc spheres centered at each subhalo\n",
    "Suppose we have four particles (ptl1, ptl2, ptl3, ptl4) in the ptldf and tow subhalos (sub1, sub2) in the subdf. If ptl1 and ptl2 are closer to sub1 than 50 kpc, ptl3 is closer to sub2 than 50 kpc, and ptl5 is closer to both sub1 and sub2 than 50 kpc, the resulting table will be\n",
    "\n",
    "|particle data|subhalo data|\n",
    "|---|---|\n",
    "|ptl1 data | sub1 data|\n",
    "|ptl2 data | sub1 data|\n",
    "|ptl3 data | sub2 data|\n",
    "|ptl4 data | sub1 data|\n",
    "|ptl4 data | sub2 data|.\n",
    "\n",
    "Specifically, we will construct the above-like table by 'joining' ptldf and 'subdf'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334348d-58e0-4e20-b086-d1bb233aa7c1",
   "metadata": {},
   "source": [
    "## 2.1. Assign subbox number to each particle and subhalo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132a3c2-dc3e-4d81-a9e7-6da4f1ced9c0",
   "metadata": {},
   "source": [
    "We first divide the simulation box into small subboxes and identify particles and subhalos belong to each small box (assign box numbers to each particle and subhalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34205b28-07d2-4cec-8b7d-691af3ed312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some constants \n",
    "id_size = 6000 # number of subboxes\n",
    "subbox_size = box_size/id_size # the size of subboxes\n",
    "half_box = box_size/2 # half subbox size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cb08d3-7c6d-4fc8-9ee0-593056fbd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign box number to each particle\n",
    "ptldf = ptldf.withColumn(\"ix\", floor(F.col(\"px\") / subbox_size ).cast('int'))\n",
    "ptldf = ptldf.withColumn(\"iy\", floor(F.col(\"py\") / subbox_size ).cast('int'))\n",
    "ptldf = ptldf.withColumn(\"iz\", floor(F.col(\"pz\") / subbox_size ).cast('int'))\n",
    "\n",
    "# .withColumn(\"a\", x): replace the values of a column \"a\" with x. If the column \"a\" does not exist, add the column named \"a\" and fill its value with x.\n",
    "# .cast(X): converts the datatype into X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6490a455-e3b8-4fac-8e08-9944b2932de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign box number to each subhalo\n",
    "subdf = subdf.withColumn(\"subhalo_ix\", floor(F.col(\"subhalo_px\") / subbox_size).cast('int'))\n",
    "subdf = subdf.withColumn(\"subhalo_iy\", floor(F.col(\"subhalo_py\") / subbox_size).cast('int'))\n",
    "subdf = subdf.withColumn(\"subhalo_iz\", floor(F.col(\"subhalo_pz\") / subbox_size).cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fde1f-24b3-475e-8f70-37f6a945fb6c",
   "metadata": {},
   "source": [
    "## 2.2. Broadcast subhalo data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd622063-3f94-4ba0-8704-cb28d27c38dc",
   "metadata": {},
   "source": [
    "ptldf and subdf are saved separted into several working nodes. Because we have to check the distances from every particle to the all subhalos, all working nodes should have the entire subdf data. We share the enteir subdf with all working nodes by following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94adfb0e-0675-4dc0-ae6e-3585284e1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_subdf = F.broadcast(subdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f281a5a-9208-4529-a05b-b2e3dd625721",
   "metadata": {},
   "source": [
    "## 2.3. 'join' based on the subbox numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7501c-791f-4170-a194-7f51c7891e31",
   "metadata": {},
   "source": [
    "Calculating distances from all particles to all subhalos requires very long computation time.\n",
    "To reduce the computation, we first divides the simulation box into small boxes and join the particles with the subhalos in the box where the particles are and the boxes next to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f38538d8-a0b9-450b-88b0-60ad7a36f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+---------+---------+---------+-----------+----+---+---+----------+----------+----------+----------+----------+----------+--------------------+----------+----------+----------+----------+\n",
      "|               px|               py|               pz|       vx|       vy|       vz|       mass|  ix| iy| iz|subhalo_px|subhalo_py|subhalo_pz|subhalo_vx|subhalo_vy|subhalo_vz|subhalo_StarHalfMass|subhalo_id|subhalo_ix|subhalo_iy|subhalo_iz|\n",
      "+-----------------+-----------------+-----------------+---------+---------+---------+-----------+----+---+---+----------+----------+----------+----------+----------+----------+--------------------+----------+----------+----------+----------+\n",
      "|59733.16062881968|18815.73022987926|3110.127319623236|-271.8078| 36.52871|462.94504|6.503996E-4|1748|550| 91|  59689.17| 18819.373| 3144.3225| -313.7507| -55.04246|  536.9568|                 0.0|    743320|      1747|       550|        92|\n",
      "|59733.16062881968|18815.73022987926|3110.127319623236|-271.8078| 36.52871|462.94504|6.503996E-4|1748|550| 91|  59712.02| 18837.266|  3114.887|  26.12387|   822.157|  -175.124|                 0.0|    743342|      1747|       551|        91|\n",
      "| 59989.8680687528|  18511.943681437|3289.819086739001|58.250034|109.10752|301.58124|5.603535E-4|1755|541| 96|  60004.79| 18491.545|  3256.901| 330.72787|  616.9033|-264.37018|                 0.0|    743225|      1756|       541|        95|\n",
      "| 59989.8680687528|  18511.943681437|3289.819086739001|58.250034|109.10752|301.58124|5.603535E-4|1755|541| 96|  59933.01|  18551.18| 3305.1763| -720.2866|-13.082733|-156.59166|                 0.0|    743304|      1754|       542|        96|\n",
      "+-----------------+-----------------+-----------------+---------+---------+---------+-----------+----+---+---+----------+----------+----------+----------+----------+----------+--------------------+----------+----------+----------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to determine if the difference between a particle's boxnumber and a subhalo's boxnumber is smaller than or equal to one.\n",
    "def int_ptl2subhalo(ptl_boxnumber, subhalo_boxnumber):\n",
    "    id_diff = F.least(F.abs(F.col(ptl_boxnumber)-F.col(subhalo_boxnumber)), id_size-F.abs(F.col(ptl_boxnumber)-F.col(subhalo_boxnumber)))\n",
    "    return id_diff <= 1\n",
    "# F.abs(F.col(ptl_boxnumber)-F.col(subhalo_boxnumber)): difference between particle and subhalo boxnumber\n",
    "# id_size-F.abs(F.col(ptl_boxnumber)-F.col(subhalo_boxnumber)): difference of boxnumber taking the simulation boundary of boxnubmer\n",
    "    \n",
    "# spark Data Frame joined based on boxnumber\n",
    "boxnumber_joined_star_ptldf = (\n",
    "    star_ptldf.alias(\"particles\").join(\n",
    "        broadcast_subdf.alias(\"subhalos\"),\n",
    "        (int_ptl2subhalo(\"particles.ix\", \"subhalos.subhalo_ix\")  &\n",
    "         int_ptl2subhalo(\"particles.iy\", \"subhalos.subhalo_iy\")  &\n",
    "         int_ptl2subhalo(\"particles.iz\", \"subhalos.subhalo_iz\"))\n",
    "        )\n",
    ")\n",
    "# df.alias(X): alias (refer to) data frame df with X.\n",
    "# df1.join(df2, condition): join df1 with df2 if condition is satisfied.\n",
    "\n",
    "boxnumber_joined_star_ptldf.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d749db-0148-42ca-afab-620da58bbd7b",
   "metadata": {},
   "source": [
    "## 2.4. Calculate the squared distance of the particles from the subhalo positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f247056-6c15-4791-832c-34e076a9c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position difference between particles and subhalos\n",
    "boxnumber_joined_star_ptldf = boxnumber_joined_star_ptldf.withColumn(\"rel_px\", F.pmod(F.col(\"particles.px\") - F.col(\"subhalos.subhalo_px\") + half_box, box_size) - half_box)\n",
    "boxnumber_joined_star_ptldf = boxnumber_joined_star_ptldf.withColumn(\"rel_py\", F.pmod(F.col(\"particles.py\") - F.col(\"subhalos.subhalo_py\") + half_box, box_size) - half_box)\n",
    "boxnumber_joined_star_ptldf = boxnumber_joined_star_ptldf.withColumn(\"rel_pz\", F.pmod(F.col(\"particles.pz\") - F.col(\"subhalos.subhalo_pz\") + half_box, box_size) - half_box)\n",
    "\n",
    "# squared distances of the particles from the subhalos\n",
    "boxnumber_joined_star_ptldf = boxnumber_joined_star_ptldf.withColumn(\"sq_dist_subhalo2ptl\", F.pow(F.col(\"rel_px\"), 2) + F.pow(F.col(\"rel_py\"), 2) + F.pow(F.col(\"rel_pz\"), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e0b17-673e-4f4b-b65e-ce37e84a7b80",
   "metadata": {},
   "source": [
    "## 2.5. Construct and save a joined data frame filtered by $d_{\\rm ptl-subhalo}<50 {\\rm ~kpc}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a07d8526-4f1f-47ac-9f19-dd9f2a798676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_radius = (50*h)**2 # 50kpc aperture size # note we position in our data frames has kpc/h unit\n",
    "joined_star_df = boxnumber_joined_star_ptldf.filter(F.col(\"sq_dist_subhalo2ptl\")<sq_radius)\n",
    "\n",
    "# joined_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274ad72-d504-415e-a077-b1b9e283cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the joined_df\n",
    "# It takes several hours. I recommend to do this task in .py file and run in the terminal.\n",
    "filename = 'hdfs://sohnic:54310/data/TNG300/snap99/subhalos2ptls_star.parquet.snappy'\n",
    "joined_star_df.write.option(\"compression\", \"snappy\").mode(\"overwrite\").parquet(filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
