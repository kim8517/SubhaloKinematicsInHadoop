{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6429c0-fd11-494c-ab21-45c816ffa428",
   "metadata": {},
   "source": [
    "Here, we measure 3D $V_{\\rm *, rot}~\\&~\\sigma_{*}$ for subhalos at z = 0 based on the particles within 50 kpc from the subhalo positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e71de-6780-4c37-8723-2f4e17148141",
   "metadata": {},
   "source": [
    "# 0. Import packages and set Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c11e736-0352-4d29-a884-8fbd907a67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pqw\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "import gc\n",
    "\n",
    "h = 0.6774\n",
    "a = 1/(1+0.62)\n",
    "box_size = 205000\n",
    "t_h = 7.786*1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fa6f1f-803a-4ad2-b22a-6506f45c9b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/30 12:14:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext   \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import broadcast, col, sqrt, pow, floor, monotonically_increasing_id, abs, pmod, least, row_number\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"spark://sohnic:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"100g\")\\\n",
    "    .config(\"spark.driver.memory\", \"100g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"hdfs://sohnic:54310/tmp/checkpoints\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 500)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.hive.filesourcePartitionFileCacheSize\", 4294967296)  # 4GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7179c-6385-4319-9e5d-d6c9897f0e85",
   "metadata": {},
   "source": [
    "# 1. Read the joined data frame into a spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44767a81-cb6b-446e-a411-7d04eb5a0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# particle data\n",
    "filename = 'hdfs://sohnic:54310/data/TNG300/snap99/subhalos2ptls.parquet.snappy'\n",
    "df = spark.read.parquet(filename)\n",
    "\n",
    "# # subhalo data to join Stellar Half Mass Radius information\n",
    "# subcat = pd.read_csv(\"subhalocat300.txt\", sep=' ')\n",
    "# subcat_type =  T.StructType([T.StructField('subhalo_id',T.IntegerType(), True),\n",
    "#                              T.StructField('StarHalfRad',T.IntegerType(), True)\n",
    "#                       ])\n",
    "# subcat = spark.createDataFrame(subcat[[\"SubfindID\", \"StarHalfRad\"]], subcat_type)\n",
    "\n",
    "# # Resulting DataFrame\n",
    "# df = df.join(subcat, \"subhalo_id\")\n",
    "\n",
    "# df.write.option(\"compression\", \"snappy\").mode(\"overwrite\").partitionBy(\"subhalo_id\").parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714a5c9-4328-4788-88fd-2eeea8f51168",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhalo_count = df.select(\"subhalo_id\").distinct().count()\n",
    "print(f\"Number of distinct subhalo IDs: {subhalo_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20962022-4e73-4587-8acb-06833042a04f",
   "metadata": {},
   "source": [
    "# 2. Define functions for stellar rotation velocity and stelar velocity dispersion measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39148807-2e62-4e42-ac91-7969d3d7dc18",
   "metadata": {},
   "source": [
    "Here, we calculate\n",
    "\n",
    "(0) $r_i=\\sqrt{\\Delta x^2_i+\\Delta y^2_i+\\Delta z^2_i},~\\hat{x}_i=\\Delta x_i/r,~\\hat{y}_i=\\Delta y_i/r,~\\hat{z}_i=\\Delta z_i/r$\n",
    "\n",
    "(1) $M_{\\rm subhalo}=\\sum_{i\\in{\\rm subhalo}}m_{i}$\n",
    "\n",
    "(2) $\\overline{\\mathbf{v}}_{\\rm subhalo}=\\sum_{i\\in{\\rm subhalo}}\\frac{m_{i}\\mathbf{v}_{i}}{M}$\n",
    "\n",
    "(3) $m_{i}\\Delta \\mathbf{v}_{k,i}=m_{i}(\\mathbf{v}_{k,i}-\\overline{\\mathbf{v}}_{{\\rm subhalo},k}),~k=x,y,z$\n",
    "\n",
    "(4) $m_{i}\\Delta v^2=m_{i}\\sum_k \\Delta \\mathbf{v}_{k,i}^2$\n",
    "\n",
    "(5) $K_{\\rm tot} = \\frac{1}{2}\\sum_im_{i}\\sum_k \\Delta \\mathbf{v}_{k,i}^2$\n",
    "\n",
    "(6) $\\sigma_{*} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2}{M}},~\\sigma_{*,x} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_x}{M}},~\\sigma_{*,y} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_y}{M}}~,\\sigma_{*,z} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_z}{M}}$\n",
    "\n",
    "(7) $\\mathbf{j}_{i}=\\Delta \\mathbf{x}_{i} \\times \\Delta \\mathbf{v}_{i}$\n",
    "\n",
    "(8) $\\mathbf{j}_{\\rm subhalo}=\\sum_{i \\in {\\rm subhalo}}\\mathbf{j}_{i}$\n",
    "\n",
    "(9) $|\\mathbf{j}|_{\\rm subhalo} = \\sqrt{\\sum_{k\\in \\{x,y,z\\}}\\mathbf{j}_{{\\rm subhalo},k}}$\n",
    "\n",
    "(10) $|\\mathbf{j}_{//\\mathbf{j}_{\\rm subhalo}, i}|=\\mathbf{j}_{i}\\cdot\\frac{\\mathbf{j}_{\\rm subhalo}}{|\\mathbf{j}_{\\rm subhalo}|}$\n",
    "\n",
    "(11) $|\\mathbf{\\Delta x}_{//\\mathbf{j}_{\\rm subhalo}, i}| = \\mathbf{x}_{i}\\cdot\\frac{\\mathbf{j}_{\\rm subhalo}}{|\\mathbf{j}_{\\rm subhalo}|}$\n",
    "\n",
    "(12) $d_{\\rm ptl-rotation~axis}=\\sqrt{|\\Delta \\mathbf{x}|^2-|\\mathbf{\\Delta x}_{//\\mathbf{j}_{\\rm subhalo}, i}|^2}$\n",
    "\n",
    "(13) $m_i|\\mathbf{\\Delta v}_{//{\\rm rotation}, i}|=|\\mathbf{j}_{//\\mathbf{j}_{\\rm subhalo}, i}|/d_{\\rm ptl-rotation~axis}$\n",
    "\n",
    "(14) $K_{\\rm rot} = \\frac{1}{2}\\sum_i \\frac{(m_i|\\mathbf{\\Delta v}_{\\perp \\mathbf{j}_{\\rm subhalo}, i}|)^2}{m_i}$\n",
    "\n",
    "(15) $V_{\\rm rot} = \\frac{\\sum_{i\\in{\\rm subhalo}}m_i|\\mathbf{\\Delta v}_{\\perp \\mathbf{j}_{\\rm subhalo}, i}|}{M_{\\rm subhalo}}$\n",
    "\n",
    "(16) $(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_x=\\hat{\\mathbf{j}}_{{\\rm subhalo},x}\\Delta z_i-\\hat{\\mathbf{j}}_{{\\rm subhalo},z}\\Delta y_i,~(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_y=\\hat{\\mathbf{j}}_{{\\rm subhalo},z}\\Delta x_i-\\hat{\\mathbf{j}}_{{\\rm subhalo},x}\\Delta z_i,~(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_z=\\hat{\\mathbf{j}}_{{\\rm subhalo},x}\\Delta y_i-\\hat{\\mathbf{j}}_{{\\rm subhalo},y}\\Delta x_i$\n",
    "\n",
    "(17) $|\\hat{\\mathbf{j}}\\times \\mathbf{r}_i|=\\sqrt{(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_x^2+(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_y^2+(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_z^2}$\n",
    "\n",
    "(18) $\\hat{\\phi}_{i,x}=\\frac{(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_x}{|\\hat{\\mathbf{j}}\\times \\mathbf{r_i}|},~\\hat{\\phi}_{i,y}=\\frac{(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r}_i)_y}{|\\hat{\\mathbf{j}}\\times \\mathbf{r_i}|},~\\hat{\\phi}_z=\\frac{(\\hat{\\mathbf{j}}_{\\rm subhalo}\\times \\mathbf{r_i})_z}{|\\hat{\\mathbf{j}}\\times \\mathbf{r_i}|}$\n",
    "\n",
    "(19) $\\hat{\\theta}_{i,x}=\\hat{\\phi}_{i,y}\\hat{z}_i-\\hat{\\phi}_{i,z}\\hat{y}_i,~\\hat{\\theta}_{i,y}=\\hat{\\phi}_{i,z}\\hat{x}_i-\\hat{\\phi}_{i,x}\\hat{z}_i,~\\hat{\\theta}_{i,z}=\\hat{\\phi}_{i,x}\\hat{y}_i-\\hat{\\phi}_{i,y}\\hat{x}_i$\n",
    "\n",
    "(20) $\\Delta v_{i,r} = \\Delta v_{i,x}\\hat{x}_i+\\Delta v_{i,y}\\hat{y}_i+\\Delta v_{i,z}\\hat{z}_i,~\\Delta v_{i,\\phi} = \\Delta v_{i,x}\\hat{\\phi}_{i,x}+\\Delta v_{i,y}\\hat{\\phi}_{i,y}+\\Delta v_{i,z}\\hat{\\phi}_{i,z},~\\Delta v_{i,\\theta} = \\Delta v_{i,x}\\hat{\\theta}_{i,x}+\\Delta v_{i,y}\\hat{\\theta}_{i,y}+\\Delta v_{i,z}\\hat{\\theta}_{i,z}$\n",
    "\n",
    "(21) $\\sigma_{*,r} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_r}{M}},~\\sigma_{*,\\phi} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_\\phi}{M}}~,\\sigma_{*,\\theta} = \\sqrt{\\sum_i \\frac{m_{i}\\Delta v^2_\\theta}{M}}$\n",
    "\n",
    "(22) $\\sigma_{*,\\phi, {\\rm no~rotation}} = \\sqrt{\\sum_i \\frac{m_{i}(\\Delta v_\\phi-V_{\\rm rot})^2}{M}}$\n",
    "\n",
    "(23) $\\sigma_{*, {\\rm no~rotation}} = \\sqrt{\\sigma_{*,r}^2+\\sigma_{*,\\phi,{\\rm no~rotation}}^2+\\sigma_{*,\\theta}^2}$\n",
    "\n",
    "(24) $\\sigma_{*, x, {\\rm no~rotation}}=\\sqrt{\\frac{\\sum_i m_i (\\Delta v_x-V_{\\rm rot}\\hat{\\phi}_{i,x})^2}{\\sum_i m_i}},~\\sigma_{*, y, {\\rm no~rotation}}=\\sqrt{\\frac{\\sum_i m_i (\\Delta v_y-V_{\\rm rot}\\hat{\\phi}_{i,y})^2}{\\sum_i m_i}}, \\sigma_{*, z, {\\rm no~rotation}}=\\sqrt{\\frac{\\sum_i m_i (\\Delta v_z-V_{\\rm rot}\\hat{\\phi}_{i,z})^2}{\\sum_i m_i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec2a76d6-5d0a-4104-a5c3-c1747dbc9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as W\n",
    "\n",
    "# Define subhalo window for partitioning by \"subhalo_id\"\n",
    "subhalo_window = W.partitionBy(\"subhalo_id\")\n",
    "\n",
    "# function to calculate 3D stellar rotation velocity and stellar velocity dispersion based on particles within aper\n",
    "def rotv_vdisp_aperture(df, aperture):\n",
    "    # filter a Spark Data Frame with d_{ptl-subhalo}<aperture.\n",
    "    if aperture:\n",
    "        distance_limit = (aperture * h) ** 2\n",
    "        filtered_df = df.filter(F.col(\"sq_dist_subhalo2ptl\") <= distance_limit)\n",
    "    else:\n",
    "        filtered_df = df\n",
    "\n",
    "    # (0)\n",
    "    filtered_df = filtered_df.withColumn(\"r\", F.sqrt(F.col(\"rel_px\")**2+F.col(\"rel_py\")**2+F.col(\"rel_pz\")**2))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_px\", F.col(\"rel_px\")/F.col(\"r\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_py\", F.col(\"rel_py\")/F.col(\"r\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_pz\", F.col(\"rel_pz\")/F.col(\"r\"))\n",
    "\n",
    "    # (1)\n",
    "    filtered_df = filtered_df.withColumn(\"mass_sum\", F.sum(\"mass\").over(subhalo_window)) # sum over each subhalo (the particle with the same subhalo_id)\n",
    "\n",
    "    # (2)\n",
    "    filtered_df = filtered_df.withColumn(\"vx_weighted\", F.col(\"mass\") * F.col(\"vx\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vy_weighted\", F.col(\"mass\") * F.col(\"vy\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vz_weighted\", F.col(\"mass\") * F.col(\"vz\"))\n",
    "    \n",
    "    filtered_df = filtered_df.withColumn(\"vx_avg\", F.sum(\"vx_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vy_avg\", F.sum(\"vy_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vz_avg\", F.sum(\"vz_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "\n",
    "    # (3)\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vx\", F.col(\"vx\") - F.col(\"vx_avg\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vy\", F.col(\"vy\") - F.col(\"vy_avg\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vz\", F.col(\"vz\") - F.col(\"vz_avg\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"vx_avg\", \"vy_avg\", \"vz_avg\")\n",
    "\n",
    "    # (4)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_x\", F.col(\"rel_vx\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_y\", F.col(\"rel_vy\")**2) \n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_z\", F.col(\"rel_vz\")**2) \n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_weighted\", F.col(\"mass\") * (F.col(\"dispersion_x\") + F.col(\"dispersion_y\") + F.col(\"dispersion_z\")))\n",
    "    \n",
    "    # (5)\n",
    "    rotv_vdisp_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"dispersion_weighted\")*0.5).alias(f\"total_kinetic_energy_{aperture}\"))\n",
    "    # .groupBy(\"a\"): groupiong the spark DataFrame based on the column \"a\"\n",
    "    # .agg(X): perfroms aggregate calculation on each group\n",
    "    \n",
    "    # (6)\n",
    "    vdisp_x_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_x\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_x\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_x_df, \"subhalo_id\")\n",
    "    vdisp_y_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_y\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_y\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_y_df, \"subhalo_id\")\n",
    "    vdisp_z_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_z\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_z\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_z_df, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.withColumn(f\"mass_weighted_velocity_dispersion_{aperture}\",\n",
    "                            F.col(f\"mass_weighted_velocity_dispersion_{aperture}_x\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_y\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_z\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"dispersion_x\", \"dispersion_y\", \"dispersion_z\", \"dispersion_weighted\")\n",
    "    \n",
    "    # (7)\n",
    "    filtered_df = filtered_df.withColumn(\"j_x\", F.col(\"mass\") * ( F.col(\"rel_py\") * F.col(\"rel_vz\") - F.col(\"rel_pz\") * F.col(\"rel_vy\") ))\n",
    "    filtered_df = filtered_df.withColumn(\"j_y\", F.col(\"mass\") * ( F.col(\"rel_pz\") * F.col(\"rel_vx\") - F.col(\"rel_px\") * F.col(\"rel_vz\") ))\n",
    "    filtered_df = filtered_df.withColumn(\"j_z\", F.col(\"mass\") * ( F.col(\"rel_px\") * F.col(\"rel_vy\") - F.col(\"rel_py\") * F.col(\"rel_vx\") ))\n",
    "\n",
    "    # (8)`\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_x\", F.sum(\"j_x\").over(subhalo_window))\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_y\", F.sum(\"j_y\").over(subhalo_window))\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_z\", F.sum(\"j_z\").over(subhalo_window))\n",
    "\n",
    "    # (9)\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot\", F.sqrt(F.col(\"j_tot_x\")*F.col(\"j_tot_x\") + F.col(\"j_tot_y\")*F.col(\"j_tot_y\") + F.col(\"j_tot_z\")*F.col(\"j_tot_z\")))\n",
    "\n",
    "    # (10)\n",
    "    filtered_df = filtered_df.withColumn(\"j_rot\", (F.col(\"j_x\")*F.col(\"j_tot_x\") + F.col(\"j_y\")*F.col(\"j_tot_y\") + F.col(\"j_z\")*F.col(\"j_tot_z\")) / F.col(\"j_tot\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"j_x\", \"j_y\", \"j_z\")\n",
    "    \n",
    "    # (11)\n",
    "    filtered_df = filtered_df.withColumn(\"R_tot\", (F.col(\"rel_px\")*F.col(\"j_tot_x\") + F.col(\"rel_py\")*F.col(\"j_tot_y\") + F.col(\"rel_pz\")*F.col(\"j_tot_z\")) / F.col(\"j_tot\"))\n",
    "    \n",
    "    # (12)\n",
    "    filtered_df = filtered_df.withColumn(\"R_rot\", F.sqrt( F.col(\"sq_dist_subhalo2ptl\") - F.col(\"R_tot\")*F.col(\"R_tot\")))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"R_tot\")\n",
    "\n",
    "    # (13)\n",
    "    filtered_df = filtered_df.filter(F.col(\"R_rot\") > 0)\n",
    "    filtered_df = filtered_df.withColumn(\"mV_rot\", F.col(\"j_rot\")/F.col(\"R_rot\"))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"R_rot\")\n",
    "\n",
    "    # (14)\n",
    "    filtered_df = filtered_df.withColumn(\"Krot\", 0.5*F.col(\"mV_rot\")**2/F.col(\"mass\"))\n",
    "    Krot_groupdf = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"Krot\")).alias(f\"rotation_kinetic_energy_{aperture}\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(Krot_groupdf, \"subhalo_id\")\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"Krot\")\n",
    "\n",
    "    # (15)\n",
    "    rotv_groupdf = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"mV_rot\") / F.max(\"mass_sum\")).alias(f\"mass_weighted_rotation_velocity_{aperture}\"))\n",
    "    filtered_df = filtered_df.join(rotv_groupdf, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(rotv_groupdf, \"subhalo_id\")\n",
    "\n",
    "\n",
    "    # (16)\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_x\", F.col(\"j_tot_x\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_y\", F.col(\"j_tot_y\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_z\", F.col(\"j_tot_z\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_x\", F.col(\"unitJ_y\")*F.col(\"rel_pz\")-F.col(\"unitJ_z\")*F.col(\"rel_py\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_y\", F.col(\"unitJ_z\")*F.col(\"rel_px\")-F.col(\"unitJ_x\")*F.col(\"rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_z\", F.col(\"unitJ_x\")*F.col(\"rel_py\")-F.col(\"unitJ_y\")*F.col(\"rel_px\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unitJ_x\", \"unitJ_y\", \"unitJ_z\")\n",
    "\n",
    "    # (17)\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec\",\n",
    "                                         F.sqrt(F.col(\"unitJcrossRvec_x\")**2+F.col(\"unitJcrossRvec_y\")**2+F.col(\"unitJcrossRvec_z\")**2))\n",
    "\n",
    "    # (18)\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_x\", F.col(\"unitJcrossRvec_x\")/F.col(\"unitJcrossRvec\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_y\", F.col(\"unitJcrossRvec_y\")/F.col(\"unitJcrossRvec\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_z\", F.col(\"unitJcrossRvec_z\")/F.col(\"unitJcrossRvec\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unitJcrossRvec_x\", \"unitJcrossRvec_y\", \"unitJcrossRvec_z\", \"unitJcrossRvec\")\n",
    "\n",
    "    # (19)\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_x\", F.col(\"unitPhi_y\")*F.col(\"unit_rel_pz\")-F.col(\"unitPhi_z\")*F.col(\"unit_rel_py\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_y\", F.col(\"unitPhi_z\")*F.col(\"unit_rel_px\")-F.col(\"unitPhi_x\")*F.col(\"unit_rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_z\", F.col(\"unitPhi_x\")*F.col(\"unit_rel_py\")-F.col(\"unitPhi_y\")*F.col(\"unit_rel_px\"))\n",
    "\n",
    "    # (20)\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vr\", F.col(\"rel_vx\")*F.col(\"unit_rel_px\")+F.col(\"rel_vy\")*F.col(\"unit_rel_py\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unit_rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vphi\", F.col(\"rel_vx\")*F.col(\"unitPhi_x\")+F.col(\"rel_vy\")*F.col(\"unitPhi_y\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unitPhi_z\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vtheta\", F.col(\"rel_vx\")*F.col(\"unitTheta_x\")+F.col(\"rel_vy\")*F.col(\"unitTheta_y\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unitTheta_z\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unit_rel_px\", \"unit_rel_py\", \"unit_rel_pz\", \"unitTheta_x\", \"unitTheta_y\", \"unitTheta_z\")\n",
    "\n",
    "    # (21)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_r\", F.col(\"rel_vr\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_phi\", F.col(\"rel_vphi\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_theta\", F.col(\"rel_vtheta\")**2)\n",
    "    \n",
    "    vdisp_r_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_r\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_r\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_r_df, \"subhalo_id\")\n",
    "    vdisp_theta_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_theta\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_theta\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_theta_df, \"subhalo_id\")\n",
    "    vdisp_phi_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_phi\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_phi\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_phi_df, \"subhalo_id\")\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"dispersion_r\", \"dispersion_phi\", \"dispersion_theta\", \"rel_vr\", \"rel_vtheta\")\n",
    "\n",
    "    # (22)\n",
    "    vdisp_phi_no_rotation_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum((F.col(\"rel_vphi\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\"))**2*F.col(\"mass\"))/ F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_phi_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_phi_no_rotation_df, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.withColumn(f\"mass_weighted_velocity_dispersion_{aperture}_no_rotation\",\n",
    "                                             F.col(f\"mass_weighted_velocity_dispersion_{aperture}_r\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_phi_no_rotation\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_theta\"))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"rel_vphi\")\n",
    "    \n",
    "    # (23)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_x_no_rotation\", (F.col(\"rel_vx\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_x\"))**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_y_no_rotation\", (F.col(\"rel_vy\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_y\"))**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_z_no_rotation\", (F.col(\"rel_vz\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_z\"))**2)\n",
    "    vdisp_x_df_no_rotation = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_x_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_x_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_x_df_no_rotation, \"subhalo_id\")\n",
    "    vdisp_y_df_no_rotation = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_y_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_y_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_y_df_no_rotation, \"subhalo_id\")\n",
    "    vdisp_z_df_no_rotation  = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_z_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_z_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_z_df_no_rotation, \"subhalo_id\")\n",
    "    \n",
    "    \n",
    "    return rotv_vdisp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cc32af2-5b43-45be-b425-a2f399524fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as W\n",
    "\n",
    "# Define subhalo window for partitioning by \"subhalo_id\"\n",
    "subhalo_window = W.partitionBy(\"subhalo_id\")\n",
    "\n",
    "# function to calculate 3D stellar rotation velocity and stellar velocity dispersion based on particles within aper\n",
    "def rotv_vdisp_column(df, column):\n",
    "    filtered_df = df.filter(F.col(\"sq_dist_subhalo2ptl\") <= (F.col(column)*h)**2)\n",
    "    aperture = column\n",
    "\n",
    "    # (0)\n",
    "    filtered_df = filtered_df.withColumn(\"r\", F.sqrt(F.col(\"rel_px\")**2+F.col(\"rel_py\")**2+F.col(\"rel_pz\")**2))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_px\", F.col(\"rel_px\")/F.col(\"r\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_py\", F.col(\"rel_py\")/F.col(\"r\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unit_rel_pz\", F.col(\"rel_pz\")/F.col(\"r\"))\n",
    "\n",
    "    # (1)\n",
    "    filtered_df = filtered_df.withColumn(\"mass_sum\", F.sum(\"mass\").over(subhalo_window)) # sum over each subhalo (the particle with the same subhalo_id)\n",
    "\n",
    "    # (2)\n",
    "    filtered_df = filtered_df.withColumn(\"vx_weighted\", F.col(\"mass\") * F.col(\"vx\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vy_weighted\", F.col(\"mass\") * F.col(\"vy\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vz_weighted\", F.col(\"mass\") * F.col(\"vz\"))\n",
    "    \n",
    "    filtered_df = filtered_df.withColumn(\"vx_avg\", F.sum(\"vx_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vy_avg\", F.sum(\"vy_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "    filtered_df = filtered_df.withColumn(\"vz_avg\", F.sum(\"vz_weighted\").over(subhalo_window) / F.col(\"mass_sum\"))\n",
    "\n",
    "    # (3)\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vx\", F.col(\"vx\") - F.col(\"vx_avg\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vy\", F.col(\"vy\") - F.col(\"vy_avg\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vz\", F.col(\"vz\") - F.col(\"vz_avg\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"vx_avg\", \"vy_avg\", \"vz_avg\")\n",
    "\n",
    "    # (4)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_x\", F.col(\"rel_vx\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_y\", F.col(\"rel_vy\")**2) \n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_z\", F.col(\"rel_vz\")**2) \n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_weighted\", F.col(\"mass\") * (F.col(\"dispersion_x\") + F.col(\"dispersion_y\") + F.col(\"dispersion_z\")))\n",
    "    \n",
    "    # (5)\n",
    "    rotv_vdisp_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"dispersion_weighted\")*0.5).alias(f\"total_kinetic_energy_{aperture}\"))\n",
    "    # .groupBy(\"a\"): groupiong the spark DataFrame based on the column \"a\"\n",
    "    # .agg(X): perfroms aggregate calculation on each group\n",
    "    \n",
    "    # (6)\n",
    "    vdisp_x_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_x\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_x\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_x_df, \"subhalo_id\")\n",
    "    vdisp_y_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_y\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_y\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_y_df, \"subhalo_id\")\n",
    "    vdisp_z_df = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_z\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_z\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_z_df, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.withColumn(f\"mass_weighted_velocity_dispersion_{aperture}\",\n",
    "                            F.col(f\"mass_weighted_velocity_dispersion_{aperture}_x\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_y\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_z\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"dispersion_x\", \"dispersion_y\", \"dispersion_z\", \"dispersion_weighted\")\n",
    "    \n",
    "    # (7)\n",
    "    filtered_df = filtered_df.withColumn(\"j_x\", F.col(\"mass\") * ( F.col(\"rel_py\") * F.col(\"rel_vz\") - F.col(\"rel_pz\") * F.col(\"rel_vy\") ))\n",
    "    filtered_df = filtered_df.withColumn(\"j_y\", F.col(\"mass\") * ( F.col(\"rel_pz\") * F.col(\"rel_vx\") - F.col(\"rel_px\") * F.col(\"rel_vz\") ))\n",
    "    filtered_df = filtered_df.withColumn(\"j_z\", F.col(\"mass\") * ( F.col(\"rel_px\") * F.col(\"rel_vy\") - F.col(\"rel_py\") * F.col(\"rel_vx\") ))\n",
    "\n",
    "    # (8)`\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_x\", F.sum(\"j_x\").over(subhalo_window))\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_y\", F.sum(\"j_y\").over(subhalo_window))\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot_z\", F.sum(\"j_z\").over(subhalo_window))\n",
    "\n",
    "    # (9)\n",
    "    filtered_df = filtered_df.withColumn(\"j_tot\", F.sqrt(F.col(\"j_tot_x\")*F.col(\"j_tot_x\") + F.col(\"j_tot_y\")*F.col(\"j_tot_y\") + F.col(\"j_tot_z\")*F.col(\"j_tot_z\")))\n",
    "\n",
    "    # (10)\n",
    "    filtered_df = filtered_df.withColumn(\"j_rot\", (F.col(\"j_x\")*F.col(\"j_tot_x\") + F.col(\"j_y\")*F.col(\"j_tot_y\") + F.col(\"j_z\")*F.col(\"j_tot_z\")) / F.col(\"j_tot\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"j_x\", \"j_y\", \"j_z\")\n",
    "    \n",
    "    # (11)\n",
    "    filtered_df = filtered_df.withColumn(\"R_tot\", (F.col(\"rel_px\")*F.col(\"j_tot_x\") + F.col(\"rel_py\")*F.col(\"j_tot_y\") + F.col(\"rel_pz\")*F.col(\"j_tot_z\")) / F.col(\"j_tot\"))\n",
    "    \n",
    "    # (12)\n",
    "    filtered_df = filtered_df.withColumn(\"R_rot\", F.sqrt( F.col(\"sq_dist_subhalo2ptl\") - F.col(\"R_tot\")*F.col(\"R_tot\")))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"R_tot\")\n",
    "\n",
    "    # (13)\n",
    "    filtered_df = filtered_df.filter(F.col(\"R_rot\") > 0)\n",
    "    filtered_df = filtered_df.withColumn(\"mV_rot\", F.col(\"j_rot\")/F.col(\"R_rot\"))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"R_rot\")\n",
    "\n",
    "    # (14)\n",
    "    filtered_df = filtered_df.withColumn(\"Krot\", 0.5*F.col(\"mV_rot\")**2/F.col(\"mass\"))\n",
    "    Krot_groupdf = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"Krot\")).alias(f\"rotation_kinetic_energy_{aperture}\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(Krot_groupdf, \"subhalo_id\")\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"Krot\")\n",
    "\n",
    "    # (15)\n",
    "    rotv_groupdf = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(\"mV_rot\") / F.max(\"mass_sum\")).alias(f\"mass_weighted_rotation_velocity_{aperture}\"))\n",
    "    filtered_df = filtered_df.join(rotv_groupdf, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(rotv_groupdf, \"subhalo_id\")\n",
    "\n",
    "\n",
    "    # (16)\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_x\", F.col(\"j_tot_x\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_y\", F.col(\"j_tot_y\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJ_z\", F.col(\"j_tot_z\")/F.col(\"j_tot\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_x\", F.col(\"unitJ_y\")*F.col(\"rel_pz\")-F.col(\"unitJ_z\")*F.col(\"rel_py\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_y\", F.col(\"unitJ_z\")*F.col(\"rel_px\")-F.col(\"unitJ_x\")*F.col(\"rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec_z\", F.col(\"unitJ_x\")*F.col(\"rel_py\")-F.col(\"unitJ_y\")*F.col(\"rel_px\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unitJ_x\", \"unitJ_y\", \"unitJ_z\")\n",
    "\n",
    "    # (17)\n",
    "    filtered_df = filtered_df.withColumn(\"unitJcrossRvec\",\n",
    "                                         F.sqrt(F.col(\"unitJcrossRvec_x\")**2+F.col(\"unitJcrossRvec_y\")**2+F.col(\"unitJcrossRvec_z\")**2))\n",
    "\n",
    "    # (18)\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_x\", F.col(\"unitJcrossRvec_x\")/F.col(\"unitJcrossRvec\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_y\", F.col(\"unitJcrossRvec_y\")/F.col(\"unitJcrossRvec\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitPhi_z\", F.col(\"unitJcrossRvec_z\")/F.col(\"unitJcrossRvec\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unitJcrossRvec_x\", \"unitJcrossRvec_y\", \"unitJcrossRvec_z\", \"unitJcrossRvec\")\n",
    "\n",
    "    # (19)\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_x\", F.col(\"unitPhi_y\")*F.col(\"unit_rel_pz\")-F.col(\"unitPhi_z\")*F.col(\"unit_rel_py\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_y\", F.col(\"unitPhi_z\")*F.col(\"unit_rel_px\")-F.col(\"unitPhi_x\")*F.col(\"unit_rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"unitTheta_z\", F.col(\"unitPhi_x\")*F.col(\"unit_rel_py\")-F.col(\"unitPhi_y\")*F.col(\"unit_rel_px\"))\n",
    "\n",
    "    # (20)\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vr\", F.col(\"rel_vx\")*F.col(\"unit_rel_px\")+F.col(\"rel_vy\")*F.col(\"unit_rel_py\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unit_rel_pz\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vphi\", F.col(\"rel_vx\")*F.col(\"unitPhi_x\")+F.col(\"rel_vy\")*F.col(\"unitPhi_y\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unitPhi_z\"))\n",
    "    filtered_df = filtered_df.withColumn(\"rel_vtheta\", F.col(\"rel_vx\")*F.col(\"unitTheta_x\")+F.col(\"rel_vy\")*F.col(\"unitTheta_y\")\n",
    "                                         +F.col(\"rel_vz\")*F.col(\"unitTheta_z\"))\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"unit_rel_px\", \"unit_rel_py\", \"unit_rel_pz\", \"unitTheta_x\", \"unitTheta_y\", \"unitTheta_z\")\n",
    "\n",
    "    # (21)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_r\", F.col(\"rel_vr\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_phi\", F.col(\"rel_vphi\")**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_theta\", F.col(\"rel_vtheta\")**2)\n",
    "    \n",
    "    vdisp_r_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_r\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_r\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_r_df, \"subhalo_id\")\n",
    "    vdisp_theta_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_theta\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_theta\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_theta_df, \"subhalo_id\")\n",
    "    vdisp_phi_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum(F.col(\"dispersion_phi\")*F.col(\"mass\")) / F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_phi\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_phi_df, \"subhalo_id\")\n",
    "\n",
    "    filtered_df = filtered_df.drop(\"dispersion_r\", \"dispersion_phi\", \"dispersion_theta\", \"rel_vr\", \"rel_vtheta\")\n",
    "\n",
    "    # (22)\n",
    "    vdisp_phi_no_rotation_df = filtered_df.groupBy(\"subhalo_id\").agg((F.sum((F.col(\"rel_vphi\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\"))**2*F.col(\"mass\"))/ F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_phi_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_phi_no_rotation_df, \"subhalo_id\")\n",
    "    rotv_vdisp_df = rotv_vdisp_df.withColumn(f\"mass_weighted_velocity_dispersion_{aperture}_no_rotation\",\n",
    "                                             F.col(f\"mass_weighted_velocity_dispersion_{aperture}_r\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_phi_no_rotation\")+F.col(f\"mass_weighted_velocity_dispersion_{aperture}_theta\"))\n",
    "    \n",
    "    filtered_df = filtered_df.drop(\"rel_vphi\")\n",
    "    \n",
    "    # (23)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_x_no_rotation\", (F.col(\"rel_vx\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_x\"))**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_y_no_rotation\", (F.col(\"rel_vy\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_y\"))**2)\n",
    "    filtered_df = filtered_df.withColumn(\"dispersion_z_no_rotation\", (F.col(\"rel_vz\")-F.col(f\"mass_weighted_rotation_velocity_{aperture}\")*F.col(\"unitPhi_z\"))**2)\n",
    "    vdisp_x_df_no_rotation = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_x_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_x_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_x_df_no_rotation, \"subhalo_id\")\n",
    "    vdisp_y_df_no_rotation = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_y_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_y_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_y_df_no_rotation, \"subhalo_id\")\n",
    "    vdisp_z_df_no_rotation  = filtered_df.groupBy(\"subhalo_id\").agg(\n",
    "        (F.sum(F.col(\"dispersion_z_no_rotation\")*F.col(\"mass\"))/F.sum(\"mass\")).alias(f\"mass_weighted_velocity_dispersion_{aperture}_z_no_rotation\"))\n",
    "    rotv_vdisp_df = rotv_vdisp_df.join(vdisp_z_df_no_rotation, \"subhalo_id\")\n",
    "    \n",
    "    return rotv_vdisp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f7ddb-af02-44ad-b201-688195f74e41",
   "metadata": {},
   "source": [
    "# 3. Measure stellar rotation velocity and velocity dispersion, convert them into the Pandas data frmae, and save them in the txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fa496e0-5aeb-4b52-bd71-a9b64a338e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command./ 15791][Stage 105:(0 + 0) / 15791]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/pandas/conversion.py\", line 358, in _collect_as_arrow\n",
      "    results = list(batch_stream)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/pandas/serializers.py\", line 55, in load_stream\n",
      "    for batch in self.serializer.load_stream(stream):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/pandas/serializers.py\", line 97, in load_stream\n",
      "    reader = pa.ipc.open_stream(stream)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 190, in open_stream\n",
      "    return RecordBatchStreamReader(source, options=options,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 52, in __init__\n",
      "    self._open(source, options=options, memory_pool=memory_pool)\n",
      "  File \"pyarrow/ipc.pxi\", line 862, in pyarrow.lib._RecordBatchStreamReader._open\n",
      "  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 103:(2683 + 192) / 15791][Stage 104:(0 + 0) / 15791][Stage 105:(0 + 0) / 15791]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:358\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/serializers.py:55\u001b[0m, in \u001b[0;36mArrowCollectSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# load the batches\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserializer\u001b[38;5;241m.\u001b[39mload_stream(stream):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/serializers.py:97\u001b[0m, in \u001b[0;36mArrowStreamSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py:190\u001b[0m, in \u001b[0;36mopen_stream\u001b[0;34m(source, options, memory_pool)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mCreate reader for Arrow streaming format.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    A reader for the given source\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRecordBatchStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py:52\u001b[0m, in \u001b[0;36mRecordBatchStreamReader.__init__\u001b[0;34m(self, source, options, memory_pool)\u001b[0m\n\u001b[1;32m     51\u001b[0m options \u001b[38;5;241m=\u001b[39m _ensure_default_ipc_read_options(options)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.pxi:862\u001b[0m, in \u001b[0;36mpyarrow.lib._RecordBatchStreamReader._open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_aperture(df, 50)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_df.join(rotv_vdisp_aperture(df, 30), \"subhalo_id\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_df.join(rotv_vdisp_aperture(df, 20), \"subhalo_id\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_df.join(rotv_vdisp_aperture(df, 10), \"subhalo_id\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_df.join(rotv_vdisp_aperture(df, 5), \"subhalo_id\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# rotv_vdisp_df = rotv_vdisp_df.join(rotv_vdisp_aperture(df, 3), \"subhalo_id\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m rotv_vdisp_df \u001b[38;5;241m=\u001b[39m rotv_vdisp_df\u001b[38;5;241m.\u001b[39mjoin(rotv_vdisp_column(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarHalfRad\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubhalo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mrotv_vdisp_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:143\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m tmp_column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))]\n\u001b[1;32m    142\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[0;32m--> 143\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_column_names\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_as_arrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m     table \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:361\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    358\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch_stream)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[43mjsocket_auth_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n\u001b[1;32m    364\u001b[0m batches \u001b[38;5;241m=\u001b[39m results[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:(3680 + 192) / 15791][Stage 104:(0 + 0) / 15791][Stage 105:(0 + 0) / 15791]\r"
     ]
    }
   ],
   "source": [
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 50)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp50.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 30)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp30.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 20)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp20.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 10)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp10.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 5)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp5.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_aperture(df, 3)\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdisp3.txt', sep=' ', index=False)\n",
    "rotv_vdisp_df = rotv_vdisp_column(df, \"StarHalfRad\")\n",
    "rotv_vdisp_df.toPandas().sort_values(by=\"subhalo_id\").to_csv('result/vdispStarHalfRad.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f7a3a-eb86-486d-a7f2-8b33635d4704",
   "metadata": {},
   "source": [
    "# 4. Breif check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa292e46-5d61-4529-8158-f22b75ed7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"rotv_vdisp.txt\", sep=\" \")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f75c2d-af93-4941-bd0e-792fa35a56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result[\"mass_weighted_rotation_velocity_StarHalfRad\"], bins=np.linspace(0,1000,100), histtype='step')\n",
    "plt.hist(result[\"mass_weighted_velocity_dispersion_StarHalfRad\"], bins=np.linspace(0,1000,100), histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a0267-ea45-468b-b48c-2e4c0389342d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
